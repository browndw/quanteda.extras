---
title: "Introduction to keyness functions in the quanteda.extras R package"
author: "David Brown"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{keyness_introduction}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

## Load the quanteda.extras package

Load the package, as well as others that we'll use in this vignette.

```{r setup, message = FALSE, error=FALSE, warning=FALSE}
library(quanteda.extras)
library(quanteda)
library(tidyverse)
```

## Prepare the data

Let's begin by preparing the data that comes with the package -- the **sample_corpus**. The corpus contains data from 8 text-types:

* Academic
* Blog
* Fiction
* Magazine
* News
* Spoken
* Television & Movie
* Web

In this way, it resembles the [Corpus of Contemporary American English](https://www.english-corpora.org/coca/). However, it contains only 50 texts from each type and each text is only about 2,500 words. Thus, it is also similar to the [Brown family of corpora](https://www1.essex.ac.uk/linguistics/external/clmt/w3c/corpus_ling/content/corpora/list/private/brown/brown.html).

Note that this data is included *only* for demonstration purposes. It was *not* compiled to be used for research.

First, we'll use to **preprocess_text( )** function to "clean" the text data. The  **preprocess_text( )** function takes the following logical (TRUE/FALSE) arguments:

* **contractions** (if set to TRUE contractions will be separated so that, for example, *can't* becomes *ca n't*)
* **hyphens** (if set to TRUE hyphens will be replaced by spaces)
* **punctuation** (if set to TRUE all punctuation marks will be exluded)
* **lower_case** (if set to TRUE all strings are converted to lower case)
* **accent_replace** (if set to TRUE accented chacaracters will be replaced by unaccented ones)
* **letters_only** (if set to TRUE strings including non-letters will be eliminated)

```{r data_prep, , message = FALSE, error=FALSE}
sc <- sample_corpus %>%
  mutate(text = preprocess_text(text))
```

Here is an example of the orignal and the changes we've made:

```{r}
word(sample_corpus$text[2], 1:20)
word(sc$text[2], 1:20)
```

Next, we'll subset the data and create two sub-copora: one of fiction texts and one of academic.

```{r subset, message = FALSE, error=FALSE, warning=FALSE}
sc_fiction <- sc %>%
  filter(str_detect(doc_id, "fic")) %>% # select the texts
  corpus() %>% # create a corpus object
  tokens(what="fastestword", remove_numbers=TRUE) %>% # tokenize
  dfm() # create a document-feature matrix (dfm)

sc_acad <- sc %>%
  filter(str_detect(doc_id, "acad")) %>% # select the texts
  corpus() %>% # create a corpus object
  tokens(what="fastestword", remove_numbers=TRUE) %>% # tokenize
  dfm() # create a document-feature matrix (dfm)
```

There are a couple of important issues to be aware of:

1. The **quanteda** package has it's own native keyness function as part of  [**quanteda.textstats**](https://cran.r-project.org/web/packages/quanteda.textstats/index.html): **textstat_keyness( )**.
1. Using the **textstat_keyness( )** function requires a slightly different workflow, but is perfectly fine if you only want to generate a basic keyness statistic.
1. The keyness functions here expand that basic functionality by adding effect sizes and other measures, as well as an implementation of **"key key words,"** which accounts for how distributed key words are in the target corpus.

## Generate a keyness table

The **keyness_table( )** takes a target and a reference **dfm**. You can also apply the ["Yates correction"](https://influentialpoints.com/Training/g-likelihood_ratio_test.htm) by setting **yates=TRUE**.

```{r keyness}
kt <- keyness_table(sc_fiction, sc_acad)
```

```{r}
knitr::kable(kt[1:10,])
```

