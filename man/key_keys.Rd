% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/keyness_functions.R
\name{key_keys}
\alias{key_keys}
\title{Key of keys calculation}
\usage{
key_keys(target_dfm, reference_dfm, threshold = 0.05, yates = FALSE)
}
\arguments{
\item{target_dfm}{The target document-feature matrix}

\item{reference_dfm}{The reference document-feature matrix}

\item{threshold}{The p-value threshold for calculating percentage of documents reaching significance}

\item{yates}{A logical value indicating whether the "Yates" correction should be performed}
}
\value{
A data.frame containing the percentage of documents reaching significance, mean keyness, and mean effect size
}
\description{
The following function is based on an idea proposed by Mike Scott
and used in his concordancer WordSmith:
https://lexically.net/downloads/version4/html/index.html?database_info.htm
Rather than summing counts from all texts in the target corpus
and comparing them to those in a reference corpus,
Scott proposes to iterate through each text in the target corpus,
calculating keyness values against the reference corpus.
Then you find how many texts reach some significance threshold.
Essentially, this is a way of accounting for distribution:
Are a few texts driving keyness values? Or many?
The function returns a data.frame that includes:
\itemize{
\item the percent of texts in the target corpus for which keyness reaches the specified threshold
\item the mean keyness value in the target
\item the standard deviation of keyness
\item the mean effect size by log ratio
Note that it is easy enough to alter the function to return other values.
}
}
